import requests
from bs4 import BeautifulSoup
import json
import os
from urllib.parse import urljoin

# Create folder
os.makedirs('imagens_laminas', exist_ok=True)

# Try to scrape UFRJ site
print("üî¨ Tentando pegar l√¢minas do UFRJ...")

urls_to_try = [
    "http://www.histo.ufrj.br/LIB/banco.htm",
    "https://www.histo.ufrj.br/LIB/banco.htm",
    "http://www.histo.ufrj.br/LIB/",
]

laminas = []
found = False

for url in urls_to_try:
    try:
        print(f"Tentando: {url}")
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Procurar por links de imagens
            images = soup.find_all('img')
            links = soup.find_all('a', href=True)
            
            print(f"  Encontradas {len(images)} imagens e {len(links)} links")
            
            for img in images[:10]:  # Primeiras 10
                src = img.get('src')
                alt = img.get('alt', 'L√¢mina')
                if src and ('histo' in src.lower() or '.jpg' in src.lower() or '.png' in src.lower()):
                    full_url = urljoin(url, src)
                    laminas.append({
                        "nome": alt[:50],
                        "url": full_url
                    })
                    print(f"    ‚úì {alt[:30]}")
                    found = True
            
            if found:
                break
    except Exception as e:
        print(f"  ‚úó Erro: {str(e)[:50]}")

# Se n√£o encontrou no UFRJ, usar as do Leeds que j√° temos
if not found or len(laminas) == 0:
    print("\n‚ö†Ô∏è UFRJ indispon√≠vel, usando imagens locais do Leeds...")
    
    # Gerar lista das imagens locais que j√° temos
    import glob
    
    local_images = sorted(glob.glob('imagens_laminas/lamina_leeds_*.png')) + sorted(glob.glob('imagens_laminas/lamina_leeds_*.jpg'))
    
    descriptions = {
        '001': 'Est√¥mago - Diagrama',
        '002': 'Est√¥mago - Gl√¢ndulas g√°stricas',
        '003': 'Intestino Delgado - Pregas circulares',
        '004': 'Intestino Delgado - Vilosidades',
        '005': 'Intestino Delgado - Enter√≥cito (TEM)',
        '006': 'Intestino Delgado - Epit√©lio',
        '007': 'F√≠gado - Fluxo sang√º√≠neo',
        '008': 'F√≠gado - Espa√ßo de Disse',
        '009': 'Ves√≠cula Biliar',
        '010': 'Ap√™ndice',
        '011': 'Plaquetas',
        '012': 'Cabelo',
        '013': 'Art√©rias - Diagrama',
        '014': 'Art√©rias (TEM)',
        '015': 'Sistema vascular',
        '016': 'Capilares - Diagrama',
        '017': 'Capilares (EM)',
        '018': 'Capilares (foto)',
        '019': 'Glom√©rulo renal',
        '020': 'Capilares fenestrados',
        '021': 'Capilares fenestrados (EM)',
        '022': 'Capilares descont√≠nuos',
    }
    
    for img_path in local_images:
        filename = os.path.basename(img_path)
        # Extract number from filename
        num = filename.split('_')[-1].split('.')[0]
        desc = descriptions.get(num, filename)
        laminas.append({
            "nome": desc,
            "url": img_path
        })

print(f"\n‚úÖ Total de l√¢minas: {len(laminas)}")

# Save as JSON for the frontend
data = {
    "source": "University of Leeds",
    "total": len(laminas),
    "laminas": laminas
}

with open('laminas_data.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, indent=2, ensure_ascii=False)

print("üíæ Dados salvos em laminas_data.json")
